model_config:
  _target_: text_only_baseline.GPT2FineTuning
  token_context_len: 8
  points_in_motor_sequence: 64
  max_chars_per_token: 20
  learning_rate: 3e-5
  num_layers: 1
  hidden_size: 256

hydra:
  launcher:
    gpus_per_node: 1
    submitit_folder: ${hydra.sweep.dir}/.submitit/%j
    timeout_min: 60
    tasks_per_node: 1
    nodes: 1
    name: ${hydra.job.name}
    partition: main

  sweeper:
    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 123
      consider_prior: true
      prior_weight: 1.0
      consider_magic_clip: true
      consider_endpoints: false
      n_startup_trials: 10
      n_ei_candidates: 24
      multivariate: false
      warn_independent_sampling: true
    _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper
    direction: minimize
    storage: null
    study_name: sweep_name
    n_trials: 20
    n_jobs: 2
    # max_failure_rate: 0.0
    params:
      model_config.learning_rate: tag(log, interval(1e-5, 1e-1))
      model_config.hidden_size: int(interval(64, 512))
      model_config.num_layers: int(interval(1, 4))

deploy:
  hostname: "mila"

defaults:
  # - override hydra/launcher: submitit_slurm
  - override hydra/launcher: submitit_local
  - override hydra/sweeper: optuna
