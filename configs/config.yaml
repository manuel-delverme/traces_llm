model_config:
  _target_: text_only_baseline.GPT2FineTuning
  token_context_len: 8
  points_in_motor_sequence: 64
  max_chars_per_token: 20
  num_layers: 1
  hidden_size: 256
  learning_rate: 3e-5

experiment:
  training_hours: 0.5
  batch_size: 8

hydra:
  launcher:
    gpus_per_node: 1
    submitit_folder: ${hydra.sweep.dir}/.submitit/%j
    timeout_min: 120
    tasks_per_node: 1
    nodes: 1
    name: ${hydra.job.name}
    partition: long

deploy:
  hostname: "mila"

defaults:
  # - override hydra/launcher: submitit_slurm
  - override hydra/launcher: submitit_local
  # - override hydra/sweeper: optuna
  # - override hydra/sweeper: wandb
