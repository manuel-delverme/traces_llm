model_config:
  _target_: text_only_baseline.GPT2FineTuning
  token_context_len: 8
  points_in_motor_sequence: 64
  max_chars_per_token: 20
  learning_rate: 3e-5
  num_layers: 1
  hidden_size: 256

training_hours: 0.5
batch_size: 8

hydra:
  launcher:
    gpus_per_node: 1
    submitit_folder: ${hydra.sweep.dir}/.submitit/%j
    timeout_min: 120
    tasks_per_node: 1
    nodes: 1
    name: ${hydra.job.name}
    partition: long

  sweeper:
    _target_: hydra_plugins.hydra_ax_sweeper.ax_sweeper.AxSweeper
    max_batch_size: null
    ax_config:
      max_trials: 10
      early_stop:
        minimize: true
        max_epochs_without_improvement: 10
        epsilon: 1.0e-05
      experiment:
        name: null
        objective_name: val_loss
        minimize: true
        parameter_constraints: null
        outcome_constraints: null
        status_quo: null
      client:
        verbose_logging: false
        random_seed: null
      is_noisy: true
      params:
        # ax.RangeParameter
        model_config.learning_rate:
          type: range
          bounds: [ 1e-5, 1e-1 ]
          value_type: float
          log_scale: true

        model_config.hidden_size:
          type: range
          bounds: [ 64, 512 ]
          value_type: int

        model_config.num_layers:
          type: range
          bounds: [ 1, 4 ]
          value_type: int


deploy:
  hostname: "mila"

defaults:
  # - override hydra/launcher: submitit_slurm
  - override hydra/launcher: submitit_local
  # - override hydra/sweeper: optuna
  - override hydra/sweeper: ax
