TEXT_PADDING_ID = 0  # TODO: Change this to the correct padding ID for your tokenizer
GPT2_VOCAB_SIZE = 50257
VOCAB_SIZE = 26 ** 3
assert VOCAB_SIZE > 1
DATASET_SIZE = 32777
MAX_CHARS_PER_TOKEN = 20
