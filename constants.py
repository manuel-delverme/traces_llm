TOKEN_CONTEXT_LEN = 16 // 2
TEXT_PADDING_ID = 0  # TODO: Change this to the correct padding ID for your tokenizer
GPT2_VOCAB_SIZE = 50257

VOCAB_SIZE = 2  # GPT2_VOCAB_SIZE // 100
POINTS_IN_MOTOR_SEQUENCE = 64
MAX_CHARS_PER_TOKEN = 20
CACHE_SIZE = 5
# DOWN_SAMPLE_DATASET_RATIO = .0005 / 410
DATASET_SIZE = 1
